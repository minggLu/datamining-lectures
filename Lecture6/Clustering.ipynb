{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering data with k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.datasets as sk_data\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "#import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = sk_data.make_blobs(n_samples=100, centers=3, n_features=30,center_box=(-10.0, 10.0),random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(X, xticklabels=False, yticklabels=False, linewidths=0,cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the pairwise distances for visualization purposes\n",
    "\n",
    "\n",
    "We can compute pairwise distances using the **sklearn.metrics** functions summarized here:\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "euclidean_dists = metrics.euclidean_distances(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data using the heatmap of pairwise distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(euclidean_dists, xticklabels=False, yticklabels=False, linewidths=0, square=True,cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering data using  k-means clustering in sklearn.cluster  http://scikit-learn.org/stable/modules/clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "kmeans.fit_predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "error = kmeans.inertia_\n",
    "\n",
    "print \"The total error of the clustering is: \", error\n",
    "print '\\nCluster labels'\n",
    "print labels\n",
    "print '\\n Cluster Centroids'\n",
    "print centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 functions in all the clustering classes, **fit()** **predict()** and **fit_predict()**.Â \n",
    "\n",
    "**fit()** is building the model from the training data (e.g. finding the\n",
    "            centroids), \n",
    "            \n",
    "**predict()** is assigning labels to test data after building\n",
    "            the model, \n",
    "            \n",
    "**fit_predict()** is doing both in the same data (e.g in\n",
    "            kmeans, it finds the centroids and assigns the labels to the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results of clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print original and cluster data\n",
    "idx = np.argsort(labels)\n",
    "rX = X[idx,:]\n",
    "sns.heatmap( rX,xticklabels=False, yticklabels=False, linewidths=0,cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Rearrange so that all same labels are consecutive\n",
    "\n",
    "#print labels\n",
    "#print labels[idx]\n",
    "rearranged_dists = euclidean_dists[idx,:][:,idx]\n",
    "sns.heatmap(rearranged_dists, xticklabels=False, yticklabels=False, linewidths=0, square=True,cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error = np.zeros(11)\n",
    "error[0] = 0;\n",
    "for k in range(1,11):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n",
    "    kmeans.fit_predict(X)\n",
    "    error[k] = kmeans.inertia_\n",
    "\n",
    "plt.plot(range(1,len(error)),error[1:])\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making this a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_clusters(X,max_clusters):\n",
    "    error = np.zeros(max_clusters+1)\n",
    "    error[0] = 0;\n",
    "    for k in range(1,max_clusters+1):\n",
    "        kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n",
    "        kmeans.fit_predict(X)\n",
    "        error[k] = kmeans.inertia_\n",
    "\n",
    "    plt.plot(range(1,len(error)),error[1:])\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Error')\n",
    "\n",
    "evaluate_clusters(X,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted Rand Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $T$ is a ground truth label assignment and $C$ the clustering. Let $a$ be: the number of pairs of elements that have the same label in $T$ and the same label in $C$. Also let $b$ be: the number of pairs of elements that have different labels in $T$ and different labels in $C$. Then the Rand Index is: \n",
    "\\begin{equation}\n",
    "\\text{RI}(T,C) = \\frac{a+b}{n\\choose 2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "However the RI score does not guarantee that random label assignments will get a value close to zero (esp. if the number of clusters is in the same order of magnitude as the number of samples). To counter this effect we can discount the expected RI $E[\\text{RI}]$ of random labelings by defining the adjusted Rand index as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ri = metrics.adjusted_rand_score(labels,y)\n",
    "print ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ri_evaluate_clusters(X,max_clusters,ground_truth):\n",
    "    ri = np.zeros(max_clusters+1)\n",
    "    ri[0] = 0;\n",
    "    for k in range(1,max_clusters+1):\n",
    "        kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n",
    "        kmeans.fit_predict(X)\n",
    "        ri[k] = metrics.adjusted_rand_score(kmeans.labels_,ground_truth)\n",
    "    plt.plot(range(1,len(ri)),ri[1:])\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Adjusted Rand Index')\n",
    "    \n",
    "ri_evaluate_clusters(X,10,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages/Disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient (**sklearn.metrics.silhouette_score**) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. Let $a$ be the mean distance between a sample and all other points in the same class and $b$ be the mean distance between a sample and all other points in the next nearest cluster. Then the \n",
    "**Silhouette Coefficient** for a clustering is:\n",
    "$$s = \\frac{b - a}{\\max(a, b)}$$\n",
    "\n",
    "\n",
    "Range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    "print sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sc_evaluate_clusters(X,max_clusters):\n",
    "    s = np.zeros(max_clusters+1)\n",
    "    s[0] = 0;\n",
    "    s[1] = 0;\n",
    "    for k in range(2,max_clusters+1):\n",
    "        kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n",
    "        kmeans.fit_predict(X)\n",
    "        s[k] = metrics.silhouette_score(X,kmeans.labels_,metric='cosine')\n",
    "    plt.plot(range(2,len(s)),s[2:])\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Adjusted Rand Index')\n",
    "    \n",
    "#sc_evaluate_clusters(X,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practicing with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 Newsgroup data  http://scikit-learn.org/stable/datasets/twenty_newsgroups.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\"\"\"\n",
    "categories = [\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "    'rec.autos',\n",
    "    'rec.sport.baseball'\n",
    "]\"\"\"\n",
    "categories = ['comp.os.ms-windows.misc', 'sci.space','rec.sport.baseball']\n",
    "news_data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print news_data.target, len(news_data.target)\n",
    "print news_data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF for text documents :  http://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=4, max_df=0.8)\n",
    "data = vectorizer.fit_transform(news_data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a large text corpus, some words will be very present (e.g. âtheâ, âaâ, âisâ in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tfâidf transform.\n",
    "\n",
    "Tf means term-frequency while tfâidf means term-frequency times inverse document-frequency. This is a originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results), that has also found good use in document classification and clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{tf}(t,d) = \\text{Number of times term }t \\text{ occurs in document } d$$\n",
    "\n",
    "If $N$ is the total number of documents in the corpus $D$ then\n",
    "\n",
    "$$\\text{idf}(t,D)=\\frac{N}{|\\{d\\in D\\mid t\\in d \\}}$$\n",
    "\n",
    "$$\\text{tf-idf}(t,d)=\\text{tf}(t,d)\\times \\text{idf}(t,D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting to know your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(data), data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1,figsize=(15,10))\n",
    "sns.heatmap(data[1:100,1:200].todense(), xticklabels=False, yticklabels=False, \n",
    "            linewidths=0, cbar=False, ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print news_data.target\n",
    "print news_data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_clusters(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ri_evaluate_clusters(data,10,news_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc_evaluate_clusters(data,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking into our clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k=3\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1)\n",
    "kmeans.fit_predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "asc_order_centroids = kmeans.cluster_centers_.argsort()#[:, ::-1]\n",
    "order_centroids = asc_order_centroids[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(k):\n",
    "    print \"Cluster %d:\" % i\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print ' %s' % terms[ind]\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for setting the style of the notebook\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../theme/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
